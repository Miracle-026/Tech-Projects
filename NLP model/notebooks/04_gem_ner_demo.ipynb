{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df9fc3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "```json\n",
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"# GEM-based Named Entity Recognition Demo\\n\",\n",
    "        \"\\n\",\n",
    "        \"This notebook demonstrates the usage of the GEM-based NER model for continual learning. We'll load sample data, train on multiple domains, evaluate performance, and visualize results.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Setup\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"import sys\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"sys.path.append(os.path.abspath('..'))  # Add project root to path\\n\",\n",
    "        \"\\n\",\n",
    "        \"from src.models.gem_ner import GEMNERAnalyzer, NERExample\\n\",\n",
    "        \"from src.utils.data_loader import load_ner_data\\n\",\n",
    "        \"from pathlib import Path\\n\",\n",
    "        \"import yaml\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"%matplotlib inline\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Load Configuration\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"config_path = '../configs/gem_config.yaml'\\n\",\n",
    "        \"with open(config_path, 'r') as f:\\n\",\n",
    "        \"    config = yaml.safe_load(f)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print('Configuration:', config)\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Initialize Model\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"model = GEMNERAnalyzer(\\n\",\n",
    "        \"    model_name=config['model_name'],\\n\",\n",
    "        \"    memory_size=config['memory_size'],\\n\",\n",
    "        \"    learning_rate=config['learning_rate'],\\n\",\n",
    "        \"    batch_size=config['batch_size'],\\n\",\n",
    "        \"    max_length=config['max_length'],\\n\",\n",
    "        \"    device=config['device'],\\n\",\n",
    "        \"    save_dir=config['save_dir'],\\n\",\n",
    "        \"    verbose=config['verbose']\\n\",\n",
    "        \")\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Load Sample Data\\n\",\n",
    "        \"\\n\",\n",
    "        \"We'll load sample NER data for the 'news' and 'medical' domains.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"data_dir = Path('../datasets/ner')\\n\",\n",
    "        \"domains = config['domains'][:2]  # Use first two domains for demo\\n\",\n",
    "        \"train_datasets = {}\\n\",\n",
    "        \"test_datasets = {}\\n\",\n",
    "        \"\\n\",\n",
    "        \"for task_id, domain in enumerate(domains):\\n\",\n",
    "        \"    train_file = data_dir / domain / 'train.json'\\n\",\n",
    "        \"    test_file = data_dir / domain / 'test.json'\\n\",\n",
    "        \"    if train_file.exists():\\n\",\n",
    "        \"        train_datasets[domain] = load_ner_data(str(train_file), task_id, domain)\\n\",\n",
    "        \"    if test_file.exists():\\n\",\n",
    "        \"        test_datasets[domain] = load_ner_data(str(test_file), task_id, domain)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print('Loaded domains:', list(train_datasets.keys()))\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Train Model\\n\",\n",
    "        \"\\n\",\n",
    "        \"Train sequentially on each domain and save checkpoints.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"metrics_history = {}\\n\",\n",
    "        \"for task_id, domain in enumerate(domains):\\n\",\n",
    "        \"    print(f'Training on {domain} (Task {task_id})')\\n\",\n",
    "        \"    metrics = model.train_task(\\n\",\n",
    "        \"        train_data=train_datasets[domain],\\n\",\n",
    "        \"        task_id=task_id,\\n\",\n",
    "        \"        domain=domain,\\n\",\n",
    "        \"        epochs=config['epochs'],\\n\",\n",
    "        \"        validation_data=test_datasets.get(domain)\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    metrics_history[domain] = metrics\\n\",\n",
    "        \"    checkpoint_path = model.save_checkpoint(f'gem_ner_{domain}_task_{task_id}.pt')\\n\",\n",
    "        \"    print(f'Checkpoint saved: {checkpoint_path}')\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Evaluate Model\\n\",\n",
    "        \"\\n\",\n",
    "        \"Evaluate on all tasks to check for catastrophic forgetting.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"results = model.evaluate_all_tasks(test_datasets)\\n\",\n",
    "        \"for task_id, metrics in results.items():\\n\",\n",
    "        \"    domain = model.task_id_to_domain.get(task_id, 'unknown')\\n\",\n",
    "        \"    print(f'Evaluation on {domain} (Task {task_id}):')\\n\",\n",
    "        \"    print(f'  Accuracy: {metrics.accuracy:.4f}')\\n\",\n",
    "        \"    print(f'  F1 Score: {metrics.f1_score:.4f}')\\n\",\n",
    "        \"    print(f'  Loss: {metrics.loss:.4f}')\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Analyze Forgetting\\n\",\n",
    "        \"\\n\",\n",
    "        \"Measure catastrophic forgetting across domains.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"forgetting_scores = model.analyze_forgetting(test_datasets)\\n\",\n",
    "        \"print('Forgetting Scores:')\\n\",\n",
    "        \"for domain, score in forgetting_scores.items():\\n\",\n",
    "        \"    print(f'  {domain}: {score:.4f}')\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Visualize Results\\n\",\n",
    "        \"\\n\",\n",
    "        \"Plot accuracy and F1 score for each domain over epochs.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(12, 5))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Plot accuracy\\n\",\n",
    "        \"plt.subplot(1, 2, 1)\\n\",\n",
    "        \"for domain, metrics in metrics_history.items():\\n\",\n",
    "        \"    accuracies = [m.accuracy for m in metrics]\\n\",\n",
    "        \"    epochs = [m.epoch + 1 for m in metrics]\\n\",\n",
    "        \"    plt.plot(epochs, accuracies, label=f'{domain} Accuracy')\\n\",\n",
    "        \"plt.xlabel('Epoch')\\n\",\n",
    "        \"plt.ylabel('Accuracy')\\n\",\n",
    "        \"plt.title('Training Accuracy per Domain')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Plot F1 score\\n\",\n",
    "        \"plt.subplot(1, 2, 2)\\n\",\n",
    "        \"for domain, metrics in metrics_history.items():\\n\",\n",
    "        \"    f1_scores = [m.f1_score for m in metrics]\\n\",\n",
    "        \"    epochs = [m.epoch + 1 for m in metrics]\\n\",\n",
    "        \"    plt.plot(epochs, f1_scores, label=f'{domain} F1 Score')\\n\",\n",
    "        \"plt.xlabel('Epoch')\\n\",\n",
    "        \"plt.ylabel('F1 Score')\\n\",\n",
    "        \"plt.title('Training F1 Score per Domain')\\n\",\n",
    "        \"plt.legend()\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.savefig('../results/plots/gem_ner_training_metrics.png')\\n\",\n",
    "        \"plt.show()\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Predict on New Text\\n\",\n",
    "        \"\\n\",\n",
    "        \"Test the model on a sample text for each domain.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"sample_texts = {\\n\",\n",
    "        \"    'news': 'Apple Inc. announced a new product launch in New York.',\\n\",\n",
    "        \"    'medical': 'The patient was diagnosed with diabetes mellitus.'\\n\",\n",
    "        \"}\\n\",\n",
    "        \"\\n\",\n",
    "        \"for domain, text in sample_texts.items():\\n\",\n",
    "        \"    task_id = model.domain_to_task_id.get(domain, 0)\\n\",\n",
    "        \"    entities = model.predict_single(text, task_id, return_labels=True)\\n\",\n",
    "        \"    print(f'Predictions for {domain}:')\\n\",\n",
    "        \"    print(f'  Text: {text}')\\n\",\n",
    "        \"    print(f'  Entities: {entities}')\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"source\": [\n",
    "        \"## Summary\\n\",\n",
    "        \"\\n\",\n",
    "        \"Get a summary of performance across domains.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"source\": [\n",
    "        \"summary = model.get_domain_summary()\\n\",\n",
    "        \"print('Domain Summary:')\\n\",\n",
    "        \"for domain, perf in summary['domain_performance'].items():\\n\",\n",
    "        \"    print(f'  {domain}:')\\n\",\n",
    "        \"    print(f'    Best Accuracy: {perf[\\\"best_accuracy\\\"]:.4f}')\\n\",\n",
    "        \"    print(f'    Best F1: {perf[\\\"best_f1\\\"]:.4f}')\\n\",\n",
    "        \"    print(f'    Task ID: {perf[\\\"task_id\\\"]}')\\n\",\n",
    "        \"    print(f'    Labels: {perf[\\\"labels\\\"]}')\"\n",
    "      ],\n",
    "      \"outputs\": []\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"language\": \"python\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.8.10\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 4\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
